"""
ğŸ¯ ìµœì¢… RAG ì±—ë´‡ ì‹œìŠ¤í…œ
í–¥ìƒëœ ë²¡í„° ì €ì¥ì†Œì™€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
"""

import os
import json
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import re

load_dotenv()

class RAGChatbot:
    def __init__(self):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ì»¬ë ‰ì…˜ ë¡œë“œ
        self.collection = self.client.get_collection("insurance_terms")
        
        # LLM ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.llm = ChatOpenAI(
            model_name=os.getenv("GPT_MODEL", "gpt-4-turbo-preview"), 
            temperature=0.7,
            openai_api_key=api_key
        )
        
        # ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
        self.load_processed_data()
        
        print(f"ğŸ“š ì»¬ë ‰ì…˜ ì •ë³´:")
        print(f"   - ì´ë¦„: {self.collection.name}")
        print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {self.collection.count()}")
        print(f"   - ë¡œë“œëœ í˜ì´ì§€: {len(self.processed_data)}ê°œ")
    
    def load_processed_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ í‚¤ì›Œë“œ ê²€ìƒ‰ì— ì‚¬ìš©"""
        try:
            with open("processed_data/ì•½ê´€_pages.json", 'r', encoding='utf-8') as f:
                self.processed_data = json.load(f)
            print(f"âœ… ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.processed_data)}í˜ì´ì§€")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            self.processed_data = []
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            return count
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return 0
    
    def chat(self, query: str) -> Dict[str, Any]:
        """Streamlitìš© ì±„íŒ… ë©”ì„œë“œ"""
        print(f"\nğŸ‘¤ ì§ˆë¬¸: {query}")
        print("-" * 60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        relevant_docs = self.hybrid_search(query, k=5)
        
        if not relevant_docs:
            return {
                "query": query,
                "answer": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": []
            }
        
        print(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
        
        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        page_groups = {}
        for doc in relevant_docs:
            page = doc['metadata'].get('page', 'Unknown')
            if page not in page_groups:
                page_groups[page] = []
            page_groups[page].append(doc)
        
        print(f"ğŸ“„ ê´€ë ¨ í˜ì´ì§€: {len(page_groups)}ê°œ")
        for page in sorted(page_groups.keys()):
            chunks_count = len(page_groups[page])
            print(f"   - í˜ì´ì§€ {page}: {chunks_count}ê°œ ì²­í¬")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”)
        context_parts = []
        for page, docs in page_groups.items():
            page_content = "\n".join([doc['content'] for doc in docs])
            context_parts.append(f"[í˜ì´ì§€ {page}]\n{page_content}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""
ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **êµ¬ì²´ì„±**: ê´€ë ¨ ì¡°í•­, ê·œì •, ì¡°ê±´ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. **ì¶œì²˜ ëª…ì‹œ**: ë‹µë³€ ê·¼ê±°ê°€ ë˜ëŠ” í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. **ì‚¬ìš©ì ì¤‘ì‹¬**: ë³´í—˜ ê°€ì…ì ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
5. **ëª…í™•ì„±**: ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”

ì§ˆë¬¸ ìœ í˜•ë³„ ëŒ€ì‘:
- **ì •ì˜/ê°œë… ì§ˆë¬¸**: ëª…í™•í•œ ì •ì˜ì™€ ì ìš© ë²”ìœ„ ì„¤ëª…
- **ì ˆì°¨/ë°©ë²• ì§ˆë¬¸**: êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ í•„ìš” ì„œë¥˜ ì•ˆë‚´
- **ì¡°ê±´/ìê²© ì§ˆë¬¸**: ì •í™•í•œ ì¡°ê±´ê³¼ ì˜ˆì™¸ ì‚¬í•­ ì„¤ëª…
- **ê¸ˆì•¡/ë³´ìƒ ì§ˆë¬¸**: êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ê³„ì‚° ë°©ë²• ì„¤ëª…

ë¬´ì˜ë¯¸í•˜ê±°ë‚˜ ë¶ˆëª…í™•í•œ ì§ˆë¬¸ì˜ ê²½ìš°:
- ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ë‹¬ë¼ê³  ìš”ì²­
- ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ ì•ˆë‚´
- ì˜ˆì‹œ ì§ˆë¬¸ì„ ì œì‹œ

ë‹µë³€:
"""
        
        try:
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ì°¸ê³  í˜ì´ì§€ ì •ë³´ ì¶”ê°€ (ê°„ì†Œí™”)
            reference_pages = sorted(page_groups.keys())
            answer += f"\n\nğŸ“„ ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, reference_pages))}"
            
            # ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            for page, docs in page_groups.items():
                for doc in docs:
                    sources.append({
                        "page": page,
                        "content": doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content']
                    })
            
            return {
                "query": query,
                "answer": answer,
                "sources": sources
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "query": query,
                "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": []
            }
    
    def chat_streaming(self, query: str):
        """Streamlitìš© ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ë©”ì„œë“œ"""
        print(f"\nğŸ‘¤ ì§ˆë¬¸: {query}")
        print("-" * 60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        relevant_docs = self.hybrid_search(query, k=5)
        
        if not relevant_docs:
            yield {
                "query": query,
                "answer": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "done": True
            }
            return
        
        print(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
        
        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        page_groups = {}
        for doc in relevant_docs:
            page = doc['metadata'].get('page', 'Unknown')
            if page not in page_groups:
                page_groups[page] = []
            page_groups[page].append(doc)
        
        print(f"ğŸ“„ ê´€ë ¨ í˜ì´ì§€: {len(page_groups)}ê°œ")
        for page in sorted(page_groups.keys()):
            chunks_count = len(page_groups[page])
            print(f"   - í˜ì´ì§€ {page}: {chunks_count}ê°œ ì²­í¬")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”)
        context_parts = []
        for page, docs in page_groups.items():
            page_content = "\n".join([doc['content'] for doc in docs])
            context_parts.append(f"[í˜ì´ì§€ {page}]\n{page_content}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""
ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **êµ¬ì²´ì„±**: ê´€ë ¨ ì¡°í•­, ê·œì •, ì¡°ê±´ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. **ì¶œì²˜ ëª…ì‹œ**: ë‹µë³€ ê·¼ê±°ê°€ ë˜ëŠ” í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. **ì‚¬ìš©ì ì¤‘ì‹¬**: ë³´í—˜ ê°€ì…ì ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
5. **ëª…í™•ì„±**: ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”

ì§ˆë¬¸ ìœ í˜•ë³„ ëŒ€ì‘:
- **ì •ì˜/ê°œë… ì§ˆë¬¸**: ëª…í™•í•œ ì •ì˜ì™€ ì ìš© ë²”ìœ„ ì„¤ëª…
- **ì ˆì°¨/ë°©ë²• ì§ˆë¬¸**: êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ í•„ìš” ì„œë¥˜ ì•ˆë‚´
- **ì¡°ê±´/ìê²© ì§ˆë¬¸**: ì •í™•í•œ ì¡°ê±´ê³¼ ì˜ˆì™¸ ì‚¬í•­ ì„¤ëª…
- **ê¸ˆì•¡/ë³´ìƒ ì§ˆë¬¸**: êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ê³„ì‚° ë°©ë²• ì„¤ëª…

ë¬´ì˜ë¯¸í•˜ê±°ë‚˜ ë¶ˆëª…í™•í•œ ì§ˆë¬¸ì˜ ê²½ìš°:
- ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ë‹¬ë¼ê³  ìš”ì²­
- ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ ì•ˆë‚´
- ì˜ˆì‹œ ì§ˆë¬¸ì„ ì œì‹œ

ë‹µë³€:
"""
        
        try:
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            full_answer = ""
            for chunk in self.llm.stream(prompt):
                if hasattr(chunk, 'content'):
                    content = chunk.content
                else:
                    content = str(chunk)
                
                full_answer += content
                
                # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì „ì†¡
                yield {
                    "query": query,
                    "answer": full_answer,
                    "sources": [],  # ë‚˜ì¤‘ì— ì¶”ê°€
                    "done": False
                }
            
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ì°¸ê³  í˜ì´ì§€ ì •ë³´ ì¶”ê°€
            reference_pages = sorted(page_groups.keys())
            full_answer += f"\n\nğŸ“„ ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, reference_pages))}"
            
            # ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            for page, docs in page_groups.items():
                for doc in docs:
                    sources.append({
                        "page": page,
                        "content": doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content']
                    })
            
            # ìµœì¢… ì™„ì„±ëœ ì‘ë‹µ ì „ì†¡
            yield {
                "query": query,
                "answer": full_answer,
                "sources": sources,
                "done": True
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            yield {
                "query": query,
                "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "done": True
            }
    
    def vector_search(self, query: str, k: int = 5) -> List[Dict]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=k
            )
            
            documents = []
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i]
                documents.append({
                    'content': doc,
                    'metadata': metadata,
                    'score': results['distances'][0][i] if 'distances' in results else 0,
                    'method': 'vector'
                })
            
            return documents
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def keyword_search(self, query: str, k: int = 5) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        query_lower = query.lower()
        keyword_matches = []
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[\wê°€-í£]+', query_lower)
        
        for page_data in self.processed_data:
            text = page_data['text'].lower()
            score = 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            for keyword in keywords:
                if keyword in text:
                    score += text.count(keyword) * 2  # í‚¤ì›Œë“œ ë§¤ì¹­ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
            
            if score > 0:
                keyword_matches.append({
                    'content': page_data['text'],
                    'metadata': {
                        'page': page_data['page'],
                        'source': page_data['source']
                    },
                    'score': score,
                    'method': 'keyword'
                })
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ kê°œ ë°˜í™˜
        keyword_matches.sort(key=lambda x: x['score'], reverse=True)
        return keyword_matches[:k]
    
    def hybrid_search(self, query: str, k: int = 5) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"""
        print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: '{query}'")
        
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(query, k)
        print(f"   ğŸ“Š ë²¡í„° ê²€ìƒ‰: {len(vector_results)}ê°œ ê²°ê³¼")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = self.keyword_search(query, k)
        print(f"   ğŸ”¤ í‚¤ì›Œë“œ ê²€ìƒ‰: {len(keyword_results)}ê°œ ê²°ê³¼")
        
        # ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        all_results = []
        seen_content = set()
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            content_hash = hash(result['content'][:100])  # ì²« 100ìë¡œ ì¤‘ë³µ íŒë‹¨
            if content_hash not in seen_content:
                all_results.append(result)
                seen_content.add(content_hash)
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì œì™¸)
        for result in keyword_results:
            content_hash = hash(result['content'][:100])
            if content_hash not in seen_content:
                all_results.append(result)
                seen_content.add(content_hash)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"   âœ… ìµœì¢… ê²°ê³¼: {len(all_results)}ê°œ")
        return all_results[:k]
    
    def ask_question(self, question: str) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        print(f"\nğŸ‘¤ ì§ˆë¬¸: {question}")
        print("-" * 60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        relevant_docs = self.hybrid_search(question, k=5)
        
        if not relevant_docs:
            return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
        
        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        page_groups = {}
        for doc in relevant_docs:
            page = doc['metadata'].get('page', 'Unknown')
            if page not in page_groups:
                page_groups[page] = []
            page_groups[page].append(doc)
        
        print(f"ğŸ“„ ê´€ë ¨ í˜ì´ì§€: {len(page_groups)}ê°œ")
        for page in sorted(page_groups.keys()):
            chunks_count = len(page_groups[page])
            print(f"   - í˜ì´ì§€ {page}: {chunks_count}ê°œ ì²­í¬")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”)
        context_parts = []
        for page, docs in page_groups.items():
            page_content = "\n".join([doc['content'] for doc in docs])
            context_parts.append(f"[í˜ì´ì§€ {page}]\n{page_content}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""
ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. **ì •í™•ì„±**: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **êµ¬ì²´ì„±**: ê´€ë ¨ ì¡°í•­, ê·œì •, ì¡°ê±´ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. **ì¶œì²˜ ëª…ì‹œ**: ë‹µë³€ ê·¼ê±°ê°€ ë˜ëŠ” í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. **ì‚¬ìš©ì ì¤‘ì‹¬**: ë³´í—˜ ê°€ì…ì ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
5. **ëª…í™•ì„±**: ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ë³µì¡í•œ ë‚´ìš©ì€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”

ì§ˆë¬¸ ìœ í˜•ë³„ ëŒ€ì‘:
- **ì •ì˜/ê°œë… ì§ˆë¬¸**: ëª…í™•í•œ ì •ì˜ì™€ ì ìš© ë²”ìœ„ ì„¤ëª…
- **ì ˆì°¨/ë°©ë²• ì§ˆë¬¸**: êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ í•„ìš” ì„œë¥˜ ì•ˆë‚´
- **ì¡°ê±´/ìê²© ì§ˆë¬¸**: ì •í™•í•œ ì¡°ê±´ê³¼ ì˜ˆì™¸ ì‚¬í•­ ì„¤ëª…
- **ê¸ˆì•¡/ë³´ìƒ ì§ˆë¬¸**: êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ê³„ì‚° ë°©ë²• ì„¤ëª…

ë¬´ì˜ë¯¸í•˜ê±°ë‚˜ ë¶ˆëª…í™•í•œ ì§ˆë¬¸ì˜ ê²½ìš°:
- ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ë‹¬ë¼ê³  ìš”ì²­
- ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ ì•ˆë‚´
- ì˜ˆì‹œ ì§ˆë¬¸ì„ ì œì‹œ

ë‹µë³€:
"""
        
        try:
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ì°¸ê³  í˜ì´ì§€ ì •ë³´ ì¶”ê°€ (ê°„ì†Œí™”)
            reference_pages = sorted(page_groups.keys())
            answer += f"\n\nğŸ“„ ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, reference_pages))}"
            
            return answer
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def main():
    """ë©”ì¸ í”„ë¡œê·¸ë¨"""
    print("ğŸ¯ ìµœì¢… RAG ì±—ë´‡ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        chatbot = RAGChatbot()
        
        print(f"\nğŸ’¬ ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ {chatbot.collection.count()}ê°œ ë¬¸ì„œ)")
        print("ğŸ’¡ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 60)
        
        while True:
            question = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()
            
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not question:
                continue
            
            answer = chatbot.ask_question(question)
            print(f"\nğŸ¤– AI: {answer}")
            
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    main()
