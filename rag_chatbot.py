"""
ğŸ“˜ ë³´í—˜ ì•½ê´€ RAG ì±—ë´‡ (VectorStore + ê°•í™” í”„ë¡¬í”„íŠ¸ ë²„ì „)
"""

import os
from typing import List, Dict, Any, Optional

from dotenv import load_dotenv
from openai import OpenAI

from vector_store import VectorStore

load_dotenv()


class RAGChatbot:
    """
    ë³´í—˜ ì•½ê´€ RAG ì±—ë´‡

    - VectorStore(ChromaDB)ì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
    - ê²€ìƒ‰ëœ ì•½ê´€ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ OpenAI ëª¨ë¸ì´ ë‹µë³€ ìƒì„±
    - Streamlit UIì—ì„œ chat_streaming() / get_collection_info() ì‚¬ìš©
    """

    def __init__(
        self,
        db_path: str = "./chroma_db",   # í˜„ì¬ VectorStoreëŠ” ë‚´ë¶€ì—ì„œ ./chroma_db ì‚¬ìš©
        collection_name: str = "insurance_terms",
        model: str = "gpt-4o-mini",
        vector_store: Optional[VectorStore] = None,
    ):
        # VectorStore ì´ˆê¸°í™” (ì´ë¯¸ êµ¬ì¶•ëœ DBë§Œ ì‚¬ìš©)
        self.vector_store = vector_store or VectorStore(collection_name=collection_name)

        # OpenAI í´ë¼ì´ì–¸íŠ¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        self.client = OpenAI(api_key=api_key)

        self.model = model
        self.last_sources: List[Dict[str, Any]] = []

    # ------------------------------------------------------------------
    # ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´ (Streamlitì—ì„œ ì‚¬ìš©)
    # ------------------------------------------------------------------
    def get_collection_info(self) -> int:
        """
        ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë¬¸ì„œ(ì²­í¬) ê°œìˆ˜ ë°˜í™˜.
        Streamlit UIì—ì„œ metricìœ¼ë¡œ ì‚¬ìš©.
        """
        try:
            return self.vector_store.get_collection_info()
        except Exception:
            return 0

    # ------------------------------------------------------------------
    # ğŸ” ê²€ìƒ‰ + ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    # ------------------------------------------------------------------
    def search_similar_docs(self, query: str, top_k: int = 5) -> str:
        """
        VectorStoreì—ì„œ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ê³ ,
        LLM í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ì„ ë§Œë“ ë‹¤.

        self.last_sources ì—ëŠ” Streamlitì—ì„œ í‘œì‹œí•  ì¶œì²˜ ì •ë³´ë¥¼ ì €ì¥í•œë‹¤.
        """
        results = self.vector_store.search_similar(query, top_k=top_k)

        if not results:
            self.last_sources = []
            return ""

        context_blocks = []
        sources: List[Dict[str, Any]] = []

        for item in results:
            # vector_store ë²„ì „ë³„ í˜¸í™˜ ì²˜ë¦¬
            text = item.get("text") or item.get("content") or ""
            meta = item.get("metadata", {})

            page = item.get("page", meta.get("page", "?"))
            source_name = item.get("source", meta.get("source", "unknown"))

            # ìœ ì‚¬ë„/ì ìˆ˜ (optional)
            score = item.get("score", item.get("similarity", None))

            sources.append(
                {
                    "content": text,
                    "page": page,
                    "source": source_name,
                    "score": float(score) if score is not None else None,
                }
            )

            header = f"[page {page} / {source_name}]"
            context_blocks.append(f"{header}\n{text}")

        self.last_sources = sources
        context = "\n\n-----\n\n".join(context_blocks)
        return context

    # ------------------------------------------------------------------
    # ğŸ§  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # ------------------------------------------------------------------
    def _build_messages(self, query: str, context: str):
        """
        ì‹œìŠ¤í…œ / ìœ ì € ë©”ì‹œì§€ ìƒì„± (ì•½ê´€ ì „ìš© ê°•í™” í”„ë¡¬í”„íŠ¸)
        """
        system_content = (
            "ë„ˆëŠ” ì‚¬ìš©ìê°€ ê°€ì§„ 'ë³´í—˜ ì•½ê´€ PDF'ì—ì„œ ë°œì·Œí•œ ë¬¸ì¥ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤.\n\n"
            "ğŸ“Œ ê·œì¹™\n"
            "1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µí•œë‹¤.\n"
            "2. ì œê³µëœ 'Context(ì•½ê´€ ë°œì·Œ)' ì•ˆì—ì„œ ì§ì ‘ì ì¸ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ìˆì„ ë•Œë§Œ ë‹µí•œë‹¤.\n"
            "   - ê·¼ê±°ê°€ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ ì „í˜€ ì—†ìœ¼ë©´ "
            "     'ì œê³µëœ ì•½ê´€ ë²”ìœ„ì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•˜ê³  ì¶”ì¸¡í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
            "3. ë‹µë³€ì€ 2~5ê°œì˜ ë¬¸ë‹¨ ë˜ëŠ” ë²ˆí˜¸ ëª©ë¡ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•œë‹¤.\n"
            "4. ê¸ˆì•¡, ì§€ê¸‰ ì—¬ë¶€, ì˜ˆì™¸ ì‚¬í•­ì„ ë§í•  ë•ŒëŠ” "
            "   'ì•½ê´€ìƒìœ¼ë¡œëŠ” ~ë¡œ ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.'ì²˜ëŸ¼ í‘œí˜„í•˜ê³ , "
            "   ì‹¤ì œ ë³´ìƒ ì—¬ë¶€ëŠ” ë³´í—˜ì‚¬ ì‹¬ì‚¬ì™€ ì„¸ë¶€ ìƒí’ˆ ì¡°ê±´ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ë¶™ì¸ë‹¤.\n"
            "5. ê°€ëŠ¥í•˜ë©´ ê·¼ê±°ê°€ ëœ ë¬¸ì¥ì´ë‚˜ ì¡°í•­ì— ëŒ€í•´ "
            "   'í˜ì´ì§€ X, â—‹â—‹ì¡°(ë˜ëŠ” í•­)'ì²˜ëŸ¼ í˜ì´ì§€ ì •ë³´ë¥¼ ì–¸ê¸‰í•œë‹¤. "
            "   ì¡°í•­ ë²ˆí˜¸ê°€ ë³´ì´ì§€ ì•Šìœ¼ë©´ í˜ì´ì§€ ì •ë³´ë§Œ ì–¸ê¸‰í•œë‹¤.\n"
            "6. ë²•ë¥ Â·ì„¸ë¬´Â·íˆ¬ìÂ·ì˜ë£Œ ë“±ì˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ í•˜ì§€ ë§ê³ , "
            "   ì•½ê´€ ë¬¸êµ¬ì˜ ì˜ë¯¸ì™€ ì ìš© ê°€ëŠ¥ì„±ë§Œ ì„¤ëª…í•œë‹¤.\n"
            "7. ì¸í„°ë„·ì´ë‚˜ ì¼ë°˜ ìƒì‹ ë“±, Context ë°–ì˜ ì™¸ë¶€ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤."
        )

        user_content = (
            f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\n\n"
            "[ì•½ê´€ ë°œì·Œ(Context)]\n"
            f"{context}\n\n"
            "ìœ„ Contextë§Œì„ ê·¼ê±°ë¡œ ìœ„ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
            "- Context ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ê¸°ë³´ë‹¤ëŠ”, í•µì‹¬ ë‚´ìš©ê³¼ ì¡°ê±´ì„ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n"
            "- ê´€ë ¨ ìˆëŠ” í˜ì´ì§€/ì¡°í•­ì´ ìˆë‹¤ë©´ ë‹µë³€ ì•ˆì—ì„œ ê°™ì´ ì–¸ê¸‰í•´ ì£¼ì„¸ìš”.\n"
        )

        return [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content},
        ]

    # ------------------------------------------------------------------
    # ğŸ’¬ ë¹„ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ (ì˜µì…˜)
    # ------------------------------------------------------------------
    def chat(self, query: str, top_k: int = 5, max_tokens: int = 800) -> Dict[str, Any]:
        """
        RAG ê¸°ë°˜ ë‹¨ì¼ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë° X)
        """
        context = self.search_similar_docs(query, top_k=top_k)

        if not context:
            return {
                "answer": "ê´€ë ¨ëœ ì•½ê´€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.",
                "sources": [],
            }

        messages = self._build_messages(query, context)

        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.2,
                max_tokens=max_tokens,
            )
            answer = completion.choices[0].message.content.strip()
            return {
                "answer": answer,
                "sources": self.last_sources,
            }
        except Exception as e:
            return {
                "answer": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "sources": [],
            }

    # ------------------------------------------------------------------
    # ğŸ’¬ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ (Streamlitì—ì„œ ì‚¬ìš©)
    # ------------------------------------------------------------------
    def chat_streaming(
        self,
        query: str,
        top_k: int = 5,
        max_tokens: int = 800,
    ):
        """
        RAG ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì œë„ˆë ˆì´í„°.

        Streamlit UIì—ì„œ:

            for chunk in chatbot.chat_streaming(user_input):
                if not chunk["done"]:
                    # chunk["answer"]ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥
                else:
                    # chunk["sources"]ë¡œ ì¶œì²˜ í‘œì‹œ
        """
        context = self.search_similar_docs(query, top_k=top_k)

        if not context:
            yield {
                "answer": "ê´€ë ¨ëœ ì•½ê´€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.",
                "done": True,
                "sources": [],
            }
            return

        messages = self._build_messages(query, context)

        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.2,
                max_tokens=max_tokens,
                stream=True,
            )

            answer = ""

            for chunk in stream:
                delta = chunk.choices[0].delta.content if chunk.choices[0].delta else ""
                if delta:
                    answer += delta
                    # ì¤‘ê°„ ìŠ¤íŠ¸ë¦¬ë° ë‹¨ê³„ì—ëŠ” sources ì—†ìŒ
                    yield {
                        "answer": answer,
                        "done": False,
                    }

            # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì‹œ ìµœì¢… answer + sources ë°˜í™˜
            yield {
                "answer": answer,
                "done": True,
                "sources": self.last_sources,
            }

        except Exception as e:
            yield {
                "answer": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "done": True,
                "sources": [],
            }


# ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    bot = RAGChatbot()
    q = "í•´ì§€ ì‹œ í™˜ê¸‰ê¸ˆì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    res = bot.chat(q)
    print("Q:", q)
    print("A:", res["answer"])
    print("Sources:", res["sources"])
