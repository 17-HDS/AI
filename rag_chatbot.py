"""
ğŸ¯ ìµœì¢… RAG ì±—ë´‡ ì‹œìŠ¤í…œ
í–¥ìƒëœ ë²¡í„° ì €ì¥ì†Œì™€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš©
"""

import os
import json
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv
import re
import tiktoken

load_dotenv()

class RAGChatbot:
    def __init__(self):
        # BGE-M3 ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ê²€ìƒ‰ ì‹œ ì‚¬ìš©)
        print("ğŸ¤– BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': 'cpu'},  # GPU ì‚¬ìš© ì‹œ 'cuda'ë¡œ ë³€ê²½ ê°€ëŠ¥
            encode_kwargs={'normalize_embeddings': True}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
        )
        print("âœ… BGE-M3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ì„ë² ë”© í•¨ìˆ˜ ì •ì˜ (ChromaDB ìµœì‹  ë²„ì „ í˜¸í™˜)
        # ChromaDB 0.4.16+ ë²„ì „ì—ì„œëŠ” input íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        class BGEEmbeddingFunction:
            def __init__(self, embeddings_model):
                self.embeddings_model = embeddings_model
            
            def name(self):
                """ChromaDBê°€ ìš”êµ¬í•˜ëŠ” name ë©”ì„œë“œ"""
                return "bge-m3"
            
            def __call__(self, input):
                """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ChromaDBìš©)"""
                if isinstance(input, str):
                    input = [input]
                return self.embeddings_model.embed_documents(input)
        
        embedding_function = BGEEmbeddingFunction(self.embeddings)
        
        # ì»¬ë ‰ì…˜ ë¡œë“œ (ê¸°ì¡´ ì»¬ë ‰ì…˜ì—ë„ embedding_function í•„ìš”)
        self.collection = self.client.get_collection(
            name="insurance_terms",
            embedding_function=embedding_function
        )
        
        # LLM ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        model_name = os.getenv("GPT_MODEL", "gpt-4o-mini")  # ë” ê²½ì œì ì¸ ëª¨ë¸ë¡œ ë³€ê²½
        self.llm = ChatOpenAI(
            model_name=model_name, 
            temperature=0.7,
            openai_api_key=api_key
        )
        
        # í† í° ì¸ì½”ë” ì´ˆê¸°í™”
        try:
            self.encoding = tiktoken.encoding_for_model(model_name)
        except:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # í† í° ì œí•œ ì„¤ì •
        self.max_context_tokens = 6000  # ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ í† í° ìˆ˜
        self.max_total_tokens = 8000  # í”„ë¡¬í”„íŠ¸ ì „ì²´ ìµœëŒ€ í† í° ìˆ˜
        
        # ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
        self.load_processed_data()
        
        print(f"ğŸ“š ì»¬ë ‰ì…˜ ì •ë³´:")
        print(f"   - ì´ë¦„: {self.collection.name}")
        print(f"   - ëª¨ë¸: {model_name}")
        print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {self.collection.count()}")
        print(f"   - ë¡œë“œëœ í˜ì´ì§€: {len(self.processed_data)}ê°œ")
        print(f"   - ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ í† í°: {self.max_context_tokens}")
    
    def load_processed_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ í‚¤ì›Œë“œ ê²€ìƒ‰ì— ì‚¬ìš©"""
        try:
            with open("processed_data/ì•½ê´€_pages.json", 'r', encoding='utf-8') as f:
                self.processed_data = json.load(f)
            print(f"âœ… ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.processed_data)}í˜ì´ì§€")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            self.processed_data = []
    
    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        try:
            return len(self.encoding.encode(text))
        except:
            # ëŒ€ëµì ìœ¼ë¡œ 4ìë‹¹ 1í† í°ìœ¼ë¡œ ê³„ì‚°
            return len(text) // 4
    
    def truncate_context(self, context: str, max_tokens: int) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í† í° ìˆ˜ì— ë§ê²Œ ìë¥´ê¸°"""
        tokens = self.encoding.encode(context)
        if len(tokens) <= max_tokens:
            return context
        
        # í† í° ìˆ˜ë¥¼ ì¤„ì„
        truncated_tokens = tokens[:max_tokens]
        truncated_text = self.encoding.decode(truncated_tokens)
        return truncated_text + "\n... (ë‚´ìš©ì´ ì˜ë ¸ìŠµë‹ˆë‹¤)"
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            return count
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return 0
    
    def chat(self, query: str) -> Dict[str, Any]:
        """Streamlitìš© ì±„íŒ… ë©”ì„œë“œ"""
        print(f"\nğŸ‘¤ ì§ˆë¬¸: {query}")
        print("-" * 60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (k=3ìœ¼ë¡œ ì¶•ì†Œ)
        relevant_docs = self.hybrid_search(query, k=3)
        
        if not relevant_docs:
            return {
                "query": query,
                "answer": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": []
            }
        
        print(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
        
        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        page_groups = {}
        for doc in relevant_docs:
            page = doc['metadata'].get('page', 'Unknown')
            if page not in page_groups:
                page_groups[page] = []
            page_groups[page].append(doc)
        
        print(f"ğŸ“„ ê´€ë ¨ í˜ì´ì§€: {len(page_groups)}ê°œ")
        for page in sorted(page_groups.keys()):
            chunks_count = len(page_groups[page])
            print(f"   - í˜ì´ì§€ {page}: {chunks_count}ê°œ ì²­í¬")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”)
        context_parts = []
        for page, docs in page_groups.items():
            page_content = "\n".join([doc['content'] for doc in docs])
            context_parts.append(f"[í˜ì´ì§€ {page}]\n{page_content}")
        
        context = "\n\n".join(context_parts)
        
        # ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ í™•ì¸ ë° ì œí•œ
        context_tokens = self.count_tokens(context)
        print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {context_tokens}")
        
        if context_tokens > self.max_context_tokens:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. {self.max_context_tokens} í† í°ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            context = self.truncate_context(context, self.max_context_tokens)
        
        # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ê·œì¹™:
1. ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…
3. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
4. í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰

ë‹µë³€:"""
        
        try:
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ì°¸ê³  í˜ì´ì§€ ì •ë³´ ì¶”ê°€ (ê°„ì†Œí™”)
            reference_pages = sorted(page_groups.keys())
            answer += f"\n\nğŸ“„ ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, reference_pages))}"
            
            # ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            for page, docs in page_groups.items():
                for doc in docs:
                    sources.append({
                        "page": page,
                        "content": doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content']
                    })
            
            return {
                "query": query,
                "answer": answer,
                "sources": sources
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "query": query,
                "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": []
            }
    
    def chat_streaming(self, query: str):
        """Streamlitìš© ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ë©”ì„œë“œ"""
        print(f"\nğŸ‘¤ ì§ˆë¬¸: {query}")
        print("-" * 60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (k=3ìœ¼ë¡œ ì¶•ì†Œ)
        relevant_docs = self.hybrid_search(query, k=3)
        
        if not relevant_docs:
            yield {
                "query": query,
                "answer": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "done": True
            }
            return
        
        print(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
        
        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        page_groups = {}
        for doc in relevant_docs:
            page = doc['metadata'].get('page', 'Unknown')
            if page not in page_groups:
                page_groups[page] = []
            page_groups[page].append(doc)
        
        print(f"ğŸ“„ ê´€ë ¨ í˜ì´ì§€: {len(page_groups)}ê°œ")
        for page in sorted(page_groups.keys()):
            chunks_count = len(page_groups[page])
            print(f"   - í˜ì´ì§€ {page}: {chunks_count}ê°œ ì²­í¬")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”)
        context_parts = []
        for page, docs in page_groups.items():
            page_content = "\n".join([doc['content'] for doc in docs])
            context_parts.append(f"[í˜ì´ì§€ {page}]\n{page_content}")
        
        context = "\n\n".join(context_parts)
        
        # ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ í™•ì¸ ë° ì œí•œ
        context_tokens = self.count_tokens(context)
        print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {context_tokens}")
        
        if context_tokens > self.max_context_tokens:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. {self.max_context_tokens} í† í°ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            context = self.truncate_context(context, self.max_context_tokens)
        
        # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ê·œì¹™:
1. ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…
3. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
4. í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰

ë‹µë³€:"""
        
        try:
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            full_answer = ""
            for chunk in self.llm.stream(prompt):
                if hasattr(chunk, 'content'):
                    content = chunk.content
                else:
                    content = str(chunk)
                
                full_answer += content
                
                # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì „ì†¡
                yield {
                    "query": query,
                    "answer": full_answer,
                    "sources": [],  # ë‚˜ì¤‘ì— ì¶”ê°€
                    "done": False
                }
            
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ì°¸ê³  í˜ì´ì§€ ì •ë³´ ì¶”ê°€
            reference_pages = sorted(page_groups.keys())
            full_answer += f"\n\nğŸ“„ ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, reference_pages))}"
            
            # ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            for page, docs in page_groups.items():
                for doc in docs:
                    sources.append({
                        "page": page,
                        "content": doc['content'][:500] + "..." if len(doc['content']) > 500 else doc['content']
                    })
            
            # ìµœì¢… ì™„ì„±ëœ ì‘ë‹µ ì „ì†¡
            yield {
                "query": query,
                "answer": full_answer,
                "sources": sources,
                "done": True
            }
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            yield {
                "query": query,
                "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "done": True
            }
    
    def vector_search(self, query: str, k: int = 5) -> List[Dict]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=k
            )
            
            documents = []
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i]
                documents.append({
                    'content': doc,
                    'metadata': metadata,
                    'score': results['distances'][0][i] if 'distances' in results else 0,
                    'method': 'vector'
                })
            
            return documents
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def keyword_search(self, query: str, k: int = 5) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        query_lower = query.lower()
        keyword_matches = []
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[\wê°€-í£]+', query_lower)
        
        for page_data in self.processed_data:
            text = page_data['text'].lower()
            score = 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            for keyword in keywords:
                if keyword in text:
                    score += text.count(keyword) * 2  # í‚¤ì›Œë“œ ë§¤ì¹­ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
            
            if score > 0:
                keyword_matches.append({
                    'content': page_data['text'],
                    'metadata': {
                        'page': page_data['page'],
                        'source': page_data['source']
                    },
                    'score': score,
                    'method': 'keyword'
                })
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ kê°œ ë°˜í™˜
        keyword_matches.sort(key=lambda x: x['score'], reverse=True)
        return keyword_matches[:k]
    
    def hybrid_search(self, query: str, k: int = 5) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"""
        print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: '{query}'")
        
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(query, k)
        print(f"   ğŸ“Š ë²¡í„° ê²€ìƒ‰: {len(vector_results)}ê°œ ê²°ê³¼")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = self.keyword_search(query, k)
        print(f"   ğŸ”¤ í‚¤ì›Œë“œ ê²€ìƒ‰: {len(keyword_results)}ê°œ ê²°ê³¼")
        
        # ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        all_results = []
        seen_content = set()
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            content_hash = hash(result['content'][:100])  # ì²« 100ìë¡œ ì¤‘ë³µ íŒë‹¨
            if content_hash not in seen_content:
                all_results.append(result)
                seen_content.add(content_hash)
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì œì™¸)
        for result in keyword_results:
            content_hash = hash(result['content'][:100])
            if content_hash not in seen_content:
                all_results.append(result)
                seen_content.add(content_hash)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"   âœ… ìµœì¢… ê²°ê³¼: {len(all_results)}ê°œ")
        return all_results[:k]
    
    def ask_question(self, question: str) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        print(f"\nğŸ‘¤ ì§ˆë¬¸: {question}")
        print("-" * 60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (k=3ìœ¼ë¡œ ì¶•ì†Œ)
        relevant_docs = self.hybrid_search(question, k=3)
        
        if not relevant_docs:
            return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬")
        
        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        page_groups = {}
        for doc in relevant_docs:
            page = doc['metadata'].get('page', 'Unknown')
            if page not in page_groups:
                page_groups[page] = []
            page_groups[page].append(doc)
        
        print(f"ğŸ“„ ê´€ë ¨ í˜ì´ì§€: {len(page_groups)}ê°œ")
        for page in sorted(page_groups.keys()):
            chunks_count = len(page_groups[page])
            print(f"   - í˜ì´ì§€ {page}: {chunks_count}ê°œ ì²­í¬")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”)
        context_parts = []
        for page, docs in page_groups.items():
            page_content = "\n".join([doc['content'] for doc in docs])
            context_parts.append(f"[í˜ì´ì§€ {page}]\n{page_content}")
        
        context = "\n\n".join(context_parts)
        
        # ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ í™•ì¸ ë° ì œí•œ
        context_tokens = self.count_tokens(context)
        print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {context_tokens}")
        
        if context_tokens > self.max_context_tokens:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. {self.max_context_tokens} í† í°ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            context = self.truncate_context(context, self.max_context_tokens)
        
        # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€
2. êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…
3. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
4. í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰

ë‹µë³€:"""
        
        try:
            print("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ì°¸ê³  í˜ì´ì§€ ì •ë³´ ì¶”ê°€ (ê°„ì†Œí™”)
            reference_pages = sorted(page_groups.keys())
            answer += f"\n\nğŸ“„ ì°¸ê³  í˜ì´ì§€: {', '.join(map(str, reference_pages))}"
            
            return answer
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def main():
    """ë©”ì¸ í”„ë¡œê·¸ë¨"""
    print("ğŸ¯ ìµœì¢… RAG ì±—ë´‡ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        chatbot = RAGChatbot()
        
        print(f"\nğŸ’¬ ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ {chatbot.collection.count()}ê°œ ë¬¸ì„œ)")
        print("ğŸ’¡ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print("=" * 60)
        
        while True:
            question = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()
            
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not question:
                continue
            
            answer = chatbot.ask_question(question)
            print(f"\nğŸ¤– AI: {answer}")
            
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    main()
