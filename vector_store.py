"""
ğŸ“˜ Step 2: ì„ë² ë”© ë° Vector DB ì €ì¥ ì‹œìŠ¤í…œ
í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì–´ ChromaDBì— ì €ì¥
"""

import json
import os
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
import tiktoken
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class VectorStore:
    def __init__(self, collection_name: str = "insurance_terms"):
        self.collection_name = collection_name
        # API í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=api_key)
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"ğŸ“š ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
            )
            print(f"ğŸ“š ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
    
    def load_processed_data(self, json_file: str) -> List[Dict]:
        """ì²˜ë¦¬ëœ JSON ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“– ë°ì´í„° ë¡œë“œ ì¤‘: {json_file}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… {len(data)}í˜ì´ì§€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return data
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def chunk_text(self, text: str, page: int, source: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # 300 í† í°ìœ¼ë¡œ ì¤„ì„
            chunk_overlap=100,  # 100 í† í° ì˜¤ë²„ë©ìœ¼ë¡œ ëŠ˜ë¦¼
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = text_splitter.split_text(text)
        
        # ì²­í¬ ë°ì´í„° ìƒì„±
        chunk_data = []
        for i, chunk in enumerate(chunks):
            if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
                chunk_data.append({
                    "content": chunk.strip(),
                    "metadata": {
                        "page": page,
                        "source": source,
                        "chunk_id": i,
                        "total_chunks": len(chunks)
                    }
                })
        
        return chunk_data
    
    def process_all_pages(self, pages_data: List[Dict]) -> List[Dict]:
        """ëª¨ë“  í˜ì´ì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        print("âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        
        all_chunks = []
        for page_data in pages_data:
            page_chunks = self.chunk_text(
                page_data["text"],
                page_data["page"],
                page_data["source"]
            )
            all_chunks.extend(page_chunks)
            
            print(f"   âœ… í˜ì´ì§€ {page_data['page']}: {len(page_chunks)}ê°œ ì²­í¬")
        
        print(f"âœ‚ï¸ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks
    
    def store_in_vector_db(self, chunks: List[Dict]):
        """ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì €ì¥"""
        print("ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘...")
        
        try:
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)
            try:
                self.client.delete_collection(self.collection_name)
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
                )
                print("ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            except:
                pass
            
            # ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ì €ì¥
            batch_size = 100
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                
                # ë°ì´í„° ì¤€ë¹„
                documents = [chunk["content"] for chunk in batch]
                metadatas = [chunk["metadata"] for chunk in batch]
                ids = [f"chunk_{i}_{j}" for j in range(len(batch))]
                
                # ë²¡í„° DBì— ì¶”ê°€
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                
                print(f"   âœ… ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì™„ë£Œ ({len(batch)}ê°œ ì²­í¬)")
            
            print(f"ğŸ’¾ ì´ {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
    
    def search_similar(self, query: str, top_k: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k
            )
            
            # ê²°ê³¼ ì •ë¦¬
            search_results = []
            for i in range(len(results['documents'][0])):
                search_results.append({
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "distance": results['distances'][0][i]
                })
            
            return search_results
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            print(f"ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´:")
            print(f"   - ì´ë¦„: {self.collection_name}")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {count}")
            
            return count
            
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return 0

def main():
    """ë©”ì¸ í”„ë¡œê·¸ë¨"""
    print("ğŸ“˜ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # 1. ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
    json_file = "processed_data/ì•½ê´€_pages.json"
    if not os.path.exists(json_file):
        print(f"âŒ ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        print("ğŸ’¡ ë¨¼ì € pdf_preprocessor.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # 2. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
    vector_manager = VectorStore()
    
    # 3. ë°ì´í„° ë¡œë“œ
    pages_data = vector_manager.load_processed_data(json_file)
    if not pages_data:
        return
    
    # 4. í…ìŠ¤íŠ¸ ì²­í‚¹
    chunks = vector_manager.process_all_pages(pages_data)
    if not chunks:
        return
    
    # 5. ë²¡í„° DBì— ì €ì¥
    vector_manager.store_in_vector_db(chunks)
    
    # 6. ì €ì¥ì†Œ ì •ë³´ í™•ì¸
    vector_manager.get_collection_info()
    
    print(f"\nğŸ‰ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: ./chroma_db")
    print(f"ğŸ“š ì»¬ë ‰ì…˜: {vector_manager.collection_name}")

if __name__ == "__main__":
    main()
