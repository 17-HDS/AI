"""
ğŸ“˜ Step 2: ì„ë² ë”© ë° Vector DB ì €ì¥ ì‹œìŠ¤í…œ
í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì–´ ChromaDBì— ì €ì¥
"""

import json
import os
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class VectorStore:
    def __init__(self, collection_name: str = "insurance_terms"):
        self.collection_name = collection_name
        
        # BGE-M3 ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
        # BGE-M3ëŠ” ë©€í‹° ì–¸ì–´ ì§€ì› ë° ë†’ì€ ì„±ëŠ¥ì˜ ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸
        self.embedding_function = SentenceTransformerEmbeddingFunction(
            model_name="BAAI/bge-m3"
        )
        print("BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
        try:
            self.client.delete_collection(name=collection_name)
            print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
        except:
            pass
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (BGE-M3 ì„ë² ë”© í•¨ìˆ˜ ì§€ì •)
        self.collection = self.client.create_collection(
            name=collection_name,
            embedding_function=self.embedding_function,
            metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ", "embedding_model": "BAAI/bge-m3"}
        )
        print(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name} (ì„ë² ë”© ëª¨ë¸: BGE-M3)")
    
    def load_processed_data(self, json_file: str) -> List[Dict]:
        """ì²˜ë¦¬ëœ JSON ë°ì´í„° ë¡œë“œ"""
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {json_file}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"{len(data)}í˜ì´ì§€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return data
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def chunk_text(self, text: str, page: int, source: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # 300 í† í°ìœ¼ë¡œ ì¤„ì„
            chunk_overlap=100,  # 100 í† í° ì˜¤ë²„ë©ìœ¼ë¡œ ëŠ˜ë¦¼
            length_function=len,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        chunks = text_splitter.split_text(text)
        
        # ì²­í¬ ë°ì´í„° ìƒì„±
        chunk_data = []
        for i, chunk in enumerate(chunks):
            if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
                chunk_data.append({
                    "content": chunk.strip(),
                    "metadata": {
                        "page": page,
                        "source": source,
                        "chunk_id": i,
                        "total_chunks": len(chunks)
                    }
                })
        
        return chunk_data
    
    def process_all_pages(self, pages_data: List[Dict]) -> List[Dict]:
        """ëª¨ë“  í˜ì´ì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        print("í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        
        all_chunks = []
        for page_data in pages_data:
            page_chunks = self.chunk_text(
                page_data["text"],
                page_data["page"],
                page_data["source"]
            )
            all_chunks.extend(page_chunks)
            
            print(f"   í˜ì´ì§€ {page_data['page']}: {len(page_chunks)}ê°œ ì²­í¬")
        
        print(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks
    
    def store_in_vector_db(self, chunks: List[Dict]):
        """ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì €ì¥"""
        print("ë²¡í„° DBì— ì €ì¥ ì¤‘...")
        
        try:
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)
            try:
                self.client.delete_collection(self.collection_name)
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ", "embedding_model": "BAAI/bge-m3"}
                )
                print("ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            except:
                pass
            
            # ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ì €ì¥
            batch_size = 100
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                
                # ë°ì´í„° ì¤€ë¹„
                documents = [chunk["content"] for chunk in batch]
                metadatas = [chunk["metadata"] for chunk in batch]
                ids = [f"chunk_{i}_{j}" for j in range(len(batch))]
                
                # ë²¡í„° DBì— ì¶”ê°€
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                
                print(f"   ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì™„ë£Œ ({len(batch)}ê°œ ì²­í¬)")
            
            print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë²¡í„° DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")
    
    def search_similar(self, query: str, top_k: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k
            )
            
            # ê²°ê³¼ ì •ë¦¬
            search_results = []
            for i in range(len(results['documents'][0])):
                search_results.append({
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "distance": results['distances'][0][i]
                })
            
            return search_results
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            print(f"ì»¬ë ‰ì…˜ ì •ë³´:")
            print(f"   - ì´ë¦„: {self.collection_name}")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {count}")
            
            return count
            
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return 0

def main():
    """ë©”ì¸ í”„ë¡œê·¸ë¨"""
    print("ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # 1. ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ (í†µí•© íŒŒì¼ ìš°ì„ , ê°œë³„ íŒŒì¼ ëŒ€ì²´)
    json_files = [
        "processed_data/all_pdfs_pages.json",  # í†µí•© íŒŒì¼ ìš°ì„ 
        "processed_data/ì•½ê´€_pages.json"        # ê°œë³„ íŒŒì¼ ëŒ€ì²´
    ]
    
    json_file = None
    for file_path in json_files:
        if os.path.exists(file_path):
            json_file = file_path
            break
    
    if not json_file:
        print(f"ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € pdf_preprocessor.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # 2. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
    vector_manager = VectorStore()
    
    # 3. ë°ì´í„° ë¡œë“œ
    pages_data = vector_manager.load_processed_data(json_file)
    if not pages_data:
        return
    
    # 4. í…ìŠ¤íŠ¸ ì²­í‚¹
    chunks = vector_manager.process_all_pages(pages_data)
    if not chunks:
        return
    
    # 5. ë²¡í„° DBì— ì €ì¥
    vector_manager.store_in_vector_db(chunks)
    
    # 6. ì €ì¥ì†Œ ì •ë³´ í™•ì¸
    vector_manager.get_collection_info()
    
    print(f"\në²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ì €ì¥ ìœ„ì¹˜: ./chroma_db")
    print(f"ì»¬ë ‰ì…˜: {vector_manager.collection_name}")

if __name__ == "__main__":
    main()
