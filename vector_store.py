"""
ğŸ“˜ Step 2: ì„ë² ë”© ë° Vector DB ì €ì¥ ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)
- ID ì¤‘ë³µ ë°©ì§€
- ChromaDB ì˜êµ¬ ì €ì¥ (persist)
- ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì œì–´ ê°€ëŠ¥
- ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ (ê±°ë¦¬ ì •ê·œí™” ì ìš©)
"""

import json
import os
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class VectorStore:
    def __init__(self, collection_name: str = "insurance_terms"):
        self.collection_name = collection_name

        # OpenAI API í‚¤ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            openai_api_key=api_key
        )

        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Persistent)
        self.client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )

        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"ğŸ“š ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}")
        except Exception:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
            )
            print(f"ğŸ“š ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")

    def load_processed_data(self, json_file: str) -> List[Dict]:
        """ì²˜ë¦¬ëœ JSON ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“– ë°ì´í„° ë¡œë“œ ì¤‘: {json_file}")
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"âœ… {len(data)}í˜ì´ì§€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return data
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return []

    def chunk_text(self, text: str, page: int, source: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""],
        )

        chunks = text_splitter.split_text(text)
        chunk_data = []
        for i, chunk in enumerate(chunks):
            if chunk.strip():
                chunk_data.append(
                    {
                        "content": chunk.strip(),
                        "metadata": {
                            "page": page,
                            "source": source,
                            "chunk_id": i,
                            "total_chunks": len(chunks),
                        },
                    }
                )
        return chunk_data

    def process_all_pages(self, pages_data: List[Dict]) -> List[Dict]:
        """ëª¨ë“  í˜ì´ì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        print("âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        all_chunks = []
        for page_data in pages_data:
            page_chunks = self.chunk_text(
                page_data["text"], page_data["page"], page_data["source"]
            )
            all_chunks.extend(page_chunks)
            print(f"   âœ… í˜ì´ì§€ {page_data['page']}: {len(page_chunks)}ê°œ ì²­í¬")
        print(f"âœ‚ï¸ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks

    def store_in_vector_db(self, chunks: List[Dict], reset: bool = True):
        """ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì €ì¥"""
        print("ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘...")

        try:
            # í•„ìš” ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            if reset:
                try:
                    self.client.delete_collection(self.collection_name)
                    self.collection = self.client.create_collection(
                        name=self.collection_name,
                        metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"},
                    )
                    print("ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

            # ì²­í¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì €ì¥
            batch_size = 50  # âœ… ì•ˆì •ì„± í–¥ìƒ
            global_counter = 0
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i : i + batch_size]
                documents = [chunk["content"] for chunk in batch]
                metadatas = [chunk["metadata"] for chunk in batch]
                ids = [f"chunk_{global_counter + k}" for k in range(len(batch))]
                global_counter += len(batch)

                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids,
                )
                print(f"   âœ… ë°°ì¹˜ {i // batch_size + 1} ì €ì¥ ì™„ë£Œ ({len(batch)}ê°œ ì²­í¬)")

            print("ğŸ’¾ ChromaDB ì˜êµ¬ ì €ì¥ ì™„ë£Œ")
            print(f"ğŸ’¾ ì´ {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ë²¡í„° DB ì €ì¥ ì˜¤ë¥˜: {str(e)}")

    def search_similar(self, query: str, top_k: int = 10) -> List[Dict]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ (distance ì •ê·œí™” í¬í•¨)"""
        try:
            results = self.collection.query(query_texts=[query], n_results=top_k)
            search_results = []
            if not results or not results["documents"]:
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            distances = results["distances"][0]
            # âœ… ê±°ë¦¬ â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ì •ê·œí™”)
            max_d = max(distances)
            min_d = min(distances)
            norm_sim = [(max_d - d) / (max_d - min_d + 1e-9) for d in distances]

            for i, doc in enumerate(results["documents"][0]):
                search_results.append(
                    {
                        "content": doc,
                        "metadata": results["metadatas"][0][i],
                        "distance": distances[i],
                        "similarity": round(norm_sim[i], 4),
                    }
                )

            return sorted(search_results, key=lambda x: x["similarity"], reverse=True)

        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            print("ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´:")
            print(f"   - ì´ë¦„: {self.collection_name}")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {count}")
            return count
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return 0


def main():
    """ë©”ì¸ í”„ë¡œê·¸ë¨"""
    print("ğŸ“˜ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹œìŠ¤í…œ")
    print("=" * 60)

    json_file = "processed_data/ì•½ê´€_pages.json"
    if not os.path.exists(json_file):
        print(f"âŒ ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        print("ğŸ’¡ ë¨¼ì € pdf_preprocessor.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    vector_manager = VectorStore()
    pages_data = vector_manager.load_processed_data(json_file)
    if not pages_data:
        return

    chunks = vector_manager.process_all_pages(pages_data)
    if not chunks:
        return

    vector_manager.store_in_vector_db(chunks, reset=True)
    vector_manager.get_collection_info()

    print("\nğŸ‰ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ!")
    print("ğŸ“ ì €ì¥ ìœ„ì¹˜: ./chroma_db")
    print(f"ğŸ“š ì»¬ë ‰ì…˜: {vector_manager.collection_name}")


if __name__ == "__main__":
    main()
