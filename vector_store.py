"""
ğŸ“˜ Step 2: ì„ë² ë”© ë° Vector DB ì €ì¥ ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)

ê¸°ëŠ¥ ìš”ì•½
- ì•½ê´€ JSON(í˜ì´ì§€ ë‹¨ìœ„)ì„ ë¡œë“œí•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
- OpenAI Embeddings(text-embedding-3-large)ë¡œ ì„ë² ë”© ê³„ì‚°
- ChromaDB PersistentClientì— ë²¡í„° + ë©”íƒ€ë°ì´í„° ì €ì¥
- ì¿¼ë¦¬ ì‹œ ë™ì¼ ì„ë² ë”© ëª¨ë¸ë¡œ ê²€ìƒ‰í•˜ì—¬ ìƒìœ„ ë¬¸ì„œ ë°˜í™˜

ì£¼ìš” í´ë˜ìŠ¤
- VectorStore: ë¡œë”©, ì²­í¬ ë¶„í• , ë²¡í„° ì €ì¥, ê²€ìƒ‰ ì „ë¶€ ë‹´ë‹¹
"""

import os
import json
from typing import List, Dict, Any, Optional

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv


# .env ë¡œë”© (OPENAI_API_KEY ë“±)
load_dotenv()


class VectorStore:
    """
    ë³´í—˜ ì•½ê´€ìš© Vector DB ë˜í¼ í´ë˜ìŠ¤.

    ì‚¬ìš© ìˆœì„œ ì˜ˆì‹œ:
        vs = VectorStore()
        pages = vs.load_pages_from_json("processed_data/ì•½ê´€_pages.json")
        chunks = vs.process_all_pages(pages)
        vs.store_in_vector_db(chunks, reset=True)
        results = vs.search_similar("ê³„ì•½ í•´ì§€í•˜ë©´ í™˜ê¸‰ê¸ˆ ì–¼ë§ˆë‚˜ ë‚˜ì™€?")
    """

    def __init__(
        self,
        collection_name: str = "insurance_terms",
        persist_dir: str = "./chroma_db",
        embedding_model: str = "text-embedding-3-large",
    ) -> None:
        self.collection_name = collection_name
        self.persist_dir = persist_dir

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤ (.env í™•ì¸).")

        # Chroma Persistent Client
        self.client = chromadb.PersistentClient(
            path=self.persist_dir,
            settings=Settings(anonymized_telemetry=False),
        )

        # LangChain OpenAI ì„ë² ë”© (ì§ì ‘ embeddings ì¸ìë¡œ ë„˜ê¸¸ ì˜ˆì •)
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            api_key=api_key,
        )

        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self.collection = self._get_or_create_collection()
        print(f"âœ… VectorStore ì´ˆê¸°í™” ì™„ë£Œ (collection='{self.collection_name}')")

    # --------------------------------------------------------------------- #
    # ë‚´ë¶€ ìœ í‹¸
    # --------------------------------------------------------------------- #

    def _get_or_create_collection(self):
        """
        ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±.
        embedding_function ì€ ì‚¬ìš©í•˜ì§€ ì•Šê³ , í•­ìƒ ì§ì ‘ embeddings ë¥¼ ë„˜ê¸´ë‹¤.
        """
        try:
            collection = self.client.get_collection(self.collection_name)
            print(f"ğŸ“‚ ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {self.collection_name}")
            return collection
        except Exception:
            print(f"ğŸ†• ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
            return self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"},
            )

    @staticmethod
    def _safe_basename(path: str) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ í™•ì¥ì ì œê±°í•œ ì•ˆì „í•œ basename ë¦¬í„´."""
        base = os.path.basename(path)
        return os.path.splitext(base)[0]

    # --------------------------------------------------------------------- #
    # 1) JSON ë¡œë”©
    # --------------------------------------------------------------------- #

    def load_pages_from_json(self, json_path: str) -> List[Dict[str, Any]]:
        """
        ì•½ê´€ JSON íŒŒì¼ì„ ë¡œë“œí•œë‹¤.
        êµ¬ì¡° ì˜ˆì‹œ:
            [
              {"page": 1, "text": "...", "source": "ì•½ê´€.pdf"},
              {"page": 2, "text": "...", "source": "ì•½ê´€.pdf"},
              ...
            ]
        """
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")

        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # ê°„ë‹¨ ê²€ì¦
        if not isinstance(data, list):
            raise ValueError("âŒ JSON ìµœìƒìœ„ êµ¬ì¡°ëŠ” list ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        print(f"ğŸ“„ JSON í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {len(data)} pages from '{json_path}'")
        return data

    # --------------------------------------------------------------------- #
    # 2) í˜ì´ì§€ â†’ ì²­í¬
    # --------------------------------------------------------------------- #

    def process_all_pages(
        self,
        pages: List[Dict[str, Any]],
        chunk_size: int = 500,
        chunk_overlap: int = 100,
    ) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ë¶™ì—¬ ë°˜í™˜í•œë‹¤.

        ë°˜í™˜ í˜•ì‹:
            [
              {
                "id": "ì•½ê´€_p1_c0",
                "text": "ì²­í¬ ë‚´ìš©...",
                "metadata": {
                    "page": 1,
                    "source": "ì•½ê´€.pdf",
                    "chunk_id": 0,
                    "total_chunks": 3,
                }
              },
              ...
            ]
        """
        if not pages:
            print("âš ï¸ process_all_pages: ì…ë ¥ í˜ì´ì§€ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return []

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""],
        )

        all_chunks: List[Dict[str, Any]] = []

        for page_entry in pages:
            page_num = page_entry.get("page")
            text = (page_entry.get("text") or "").strip()
            source = page_entry.get("source") or "unknown"

            if not text:
                continue

            chunks = splitter.split_text(text)
            total_chunks = len(chunks)
            base = self._safe_basename(source)

            for idx, chunk_text in enumerate(chunks):
                chunk_id = f"{base}_p{page_num}_c{idx}"

                metadata = {
                    "page": page_num,
                    "source": source,
                    "chunk_id": idx,
                    "total_chunks": total_chunks,
                }

                all_chunks.append(
                    {
                        "id": chunk_id,
                        "text": chunk_text,
                        "metadata": metadata,
                    }
                )

        print(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: ì´ {len(all_chunks)} chunks ìƒì„±")
        return all_chunks

    # --------------------------------------------------------------------- #
    # 3) Vector DB ì €ì¥
    # --------------------------------------------------------------------- #

    def store_in_vector_db(
        self,
        chunks: List[Dict[str, Any]],
        reset: bool = False,
        batch_size: int = 50,
    ) -> None:
        """
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ê³„ì‚° í›„ ChromaDB ì»¬ë ‰ì…˜ì— ì €ì¥í•œë‹¤.

        - reset=True ì´ë©´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±
        - batch_size ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ embeddings + add ìˆ˜í–‰
        """
        if not chunks:
            print("âš ï¸ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # í•„ìš”í•˜ë©´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì»¬ë ‰ì…˜ ì¬ìƒì„±
        if reset:
            try:
                self.client.delete_collection(self.collection_name)
                print("ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œí•¨): {e}")
            finally:
                self.collection = self._get_or_create_collection()

        total = len(chunks)
        print(f"ğŸ’¾ ë²¡í„° ì €ì¥ ì‹œì‘: ì´ {total} chunks (batch_size={batch_size})")

        for start in range(0, total, batch_size):
            end = min(start + batch_size, total)
            batch = chunks[start:end]

            texts = [c["text"] for c in batch]
            metadatas = [c["metadata"] for c in batch]
            ids = [c["id"] for c in batch]

            try:
                # 1) ì„ë² ë”© ê³„ì‚°
                vectors = self.embeddings.embed_documents(texts)

                # 2) ì»¬ë ‰ì…˜ì— ì €ì¥
                self.collection.add(
                    ids=ids,
                    documents=texts,
                    metadatas=metadatas,
                    embeddings=vectors,
                )

                print(f"  ğŸ”¹ ì €ì¥ ì™„ë£Œ: {start} ~ {end - 1} (ëˆ„ì  {end}/{total})")
            except Exception as e:
                print(f"âŒ batch {start}~{end} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

        print("âœ… ëª¨ë“  ì²­í¬ ë²¡í„° ì €ì¥ ì™„ë£Œ!")

    # --------------------------------------------------------------------- #
    # 4) ê²€ìƒ‰ (RAGì—ì„œ ì§ì ‘ ì‚¬ìš©)
    # --------------------------------------------------------------------- #

    def search_similar(
        self,
        query: str,
        top_k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ ë¬¸ì¥ì„ ì„ë² ë”© í›„, ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ top_kê°œë¥¼ ë°˜í™˜í•œë‹¤.

        ë°˜í™˜ í˜•ì‹:
            [
              {
                "text": "...",
                "page": 12,
                "source": "ì•½ê´€.pdf",
                "score": 0.87,  # 0~1 (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
                "metadata": {...}
              },
              ...
            ]
        """
        if not query.strip():
            return []

        # ì¿¼ë¦¬ ì„ë² ë”© ê³„ì‚°
        query_vec = self.embeddings.embed_query(query)

        try:
            results = self.collection.query(
                query_embeddings=[query_vec],
                n_results=top_k,
                include=["documents", "metadatas", "distances"],
            )
        except Exception as e:
            print(f"âŒ search_similar ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
            return []

        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]
        dists = results.get("distances", [[]])[0]

        if not docs:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ê±°ë¦¬(distance)ë¥¼ 0~1 ì‚¬ì´ì˜ ì ìˆ˜(score)ë¡œ ì •ê·œí™” (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
        max_dist = max(dists) if dists else 1.0
        if max_dist == 0:
            max_dist = 1.0

        scored_items = []
        for doc, meta, dist in zip(docs, metas, dists):
            score = 1.0 - (dist / max_dist)
            scored_items.append(
                {
                    "text": doc,
                    "page": meta.get("page"),
                    "source": meta.get("source"),
                    "score": float(score),
                    "metadata": meta,
                }
            )

        # score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        scored_items.sort(key=lambda x: x["score"], reverse=True)
        return scored_items

    # --------------------------------------------------------------------- #
    # 5) ë””ë²„ê·¸/ì •ë³´ í•¨ìˆ˜
    # --------------------------------------------------------------------- #

    def get_collection_info(self) -> int:
        """
        ì»¬ë ‰ì…˜ì— ì €ì¥ëœ ë¬¸ì„œ(ì²­í¬) ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ê³ ,
        ê°„ë‹¨í•œ ìš”ì•½ ë¡œê·¸ë¥¼ ì¶œë ¥í•œë‹¤.
        """
        info = self.collection.get()
        num = len(info.get("ids", []))
        print(f"ğŸ“Š ì»¬ë ‰ì…˜ '{self.collection_name}' ë¬¸ì„œ ìˆ˜: {num}")
        return num


# ------------------------------------------------------------------------- #
#  ë‹¨ë… ì‹¤í–‰ìš© main (í…ŒìŠ¤íŠ¸ ìš©ë„)
# ------------------------------------------------------------------------- #

def main():
    """
    python vector_store.py ë¥¼ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œ:
    - processed_data/ì•½ê´€_pages.json ì„ ì½ì–´ì„œ
    - ì²­í¬ ìƒì„± í›„
    - ë²¡í„° DBë¥¼ reset í•˜ê³  ë‹¤ì‹œ ë¹Œë“œ
    """
    json_file = "processed_data/ì•½ê´€_pages.json"

    vs = VectorStore(
        collection_name="insurance_terms",
        persist_dir="./chroma_db",
        embedding_model="text-embedding-3-large",
    )

    pages = vs.load_pages_from_json(json_file)
    chunks = vs.process_all_pages(pages)

    if not chunks:
        print("âš ï¸ ìƒì„±ëœ ì²­í¬ê°€ ì—†ì–´ ë²¡í„° ì €ì¥ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    vs.store_in_vector_db(chunks, reset=True)
    vs.get_collection_info()

    print("\nğŸ‰ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {vs.persist_dir}")
    print(f"ğŸ“š ì»¬ë ‰ì…˜: {vs.collection_name}")


if __name__ == "__main__":
    main()
