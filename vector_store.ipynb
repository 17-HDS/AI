{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“˜ Step 2: ì„ë² ë”© ë° Vector DB ì €ì¥ ì‹œìŠ¤í…œ\n",
        "\n",
        "í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì–´ ChromaDBì— ì €ì¥\n",
        "\n",
        "## ì‚¬ìš© ë°©ë²•\n",
        "1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì•„ë˜ ì…€ ì‹¤í–‰)\n",
        "2. OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)\n",
        "3. `pdf_preprocessor.ipynb`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„\n",
        "4. ëª¨ë“  ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Google Colabìš©)\n",
        "!pip install chromadb langchain langchain-openai tiktoken python-dotenv -q\n",
        "\n",
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import tiktoken\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAI API í‚¤ ì„¤ì • (Colabì—ì„œ ì‚¬ìš©)\n",
        "# ë°©ë²• 1: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ (ê¶Œì¥)\n",
        "import os\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# ë°©ë²• 2: ì§ì ‘ ì…ë ¥ (ë³´ì•ˆ ì£¼ì˜!)\n",
        "# api_key = \"your-api-key-here\"\n",
        "\n",
        "if not api_key:\n",
        "    print(\"âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ’¡ ì•„ë˜ ì…€ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ê±°ë‚˜, í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\")\n",
        "else:\n",
        "    print(\"âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorStore:\n",
        "    def __init__(self, collection_name: str = \"insurance_terms\", api_key: str = None, chroma_db_path: str = \"./chroma_db\"):\n",
        "        self.collection_name = collection_name\n",
        "        \n",
        "        # API í‚¤ ì„¤ì •\n",
        "        if api_key is None:\n",
        "            api_key = os.getenv('OPENAI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "        \n",
        "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=api_key)\n",
        "        \n",
        "        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ê²½ë¡œ ì§€ì • ê°€ëŠ¥)\n",
        "        os.makedirs(chroma_db_path, exist_ok=True)\n",
        "        self.client = chromadb.PersistentClient(\n",
        "            path=chroma_db_path,\n",
        "            settings=Settings(anonymized_telemetry=False)\n",
        "        )\n",
        "        \n",
        "        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=collection_name)\n",
        "            print(f\"ğŸ“š ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {collection_name}\")\n",
        "        except:\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=collection_name,\n",
        "                metadata={\"description\": \"ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ\"}\n",
        "            )\n",
        "            print(f\"ğŸ“š ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}\")\n",
        "    \n",
        "    def load_processed_data(self, json_file: str) -> List[Dict]:\n",
        "        \"\"\"ì²˜ë¦¬ëœ JSON ë°ì´í„° ë¡œë“œ\"\"\"\n",
        "        print(f\"ğŸ“– ë°ì´í„° ë¡œë“œ ì¤‘: {json_file}\")\n",
        "        \n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            \n",
        "            print(f\"âœ… {len(data)}í˜ì´ì§€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
        "            return data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    def chunk_text(self, text: str, page: int, source: str) -> List[Dict]:\n",
        "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
        "        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=300,  # 300 í† í°ìœ¼ë¡œ ì¤„ì„\n",
        "            chunk_overlap=100,  # 100 í† í° ì˜¤ë²„ë©ìœ¼ë¡œ ëŠ˜ë¦¼\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        \n",
        "        # í…ìŠ¤íŠ¸ ë¶„í• \n",
        "        chunks = text_splitter.split_text(text)\n",
        "        \n",
        "        # ì²­í¬ ë°ì´í„° ìƒì„±\n",
        "        chunk_data = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸\n",
        "                chunk_data.append({\n",
        "                    \"content\": chunk.strip(),\n",
        "                    \"metadata\": {\n",
        "                        \"page\": page,\n",
        "                        \"source\": source,\n",
        "                        \"chunk_id\": i,\n",
        "                        \"total_chunks\": len(chunks)\n",
        "                    }\n",
        "                })\n",
        "        \n",
        "        return chunk_data\n",
        "    \n",
        "    def process_all_pages(self, pages_data: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"ëª¨ë“  í˜ì´ì§€ë¥¼ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
        "        print(\"âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...\")\n",
        "        \n",
        "        all_chunks = []\n",
        "        for page_data in pages_data:\n",
        "            page_chunks = self.chunk_text(\n",
        "                page_data[\"text\"],\n",
        "                page_data[\"page\"],\n",
        "                page_data[\"source\"]\n",
        "            )\n",
        "            all_chunks.extend(page_chunks)\n",
        "            \n",
        "            print(f\"   âœ… í˜ì´ì§€ {page_data['page']}: {len(page_chunks)}ê°œ ì²­í¬\")\n",
        "        \n",
        "        print(f\"âœ‚ï¸ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ\")\n",
        "        return all_chunks\n",
        "    \n",
        "    def store_in_vector_db(self, chunks: List[Dict]):\n",
        "        \"\"\"ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì €ì¥\"\"\"\n",
        "        print(\"ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘...\")\n",
        "        \n",
        "        try:\n",
        "            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)\n",
        "            try:\n",
        "                self.client.delete_collection(self.collection_name)\n",
        "                self.collection = self.client.create_collection(\n",
        "                    name=self.collection_name,\n",
        "                    metadata={\"description\": \"ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ\"}\n",
        "                )\n",
        "                print(\"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\")\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            # ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ì €ì¥\n",
        "            batch_size = 100\n",
        "            for i in range(0, len(chunks), batch_size):\n",
        "                batch = chunks[i:i + batch_size]\n",
        "                \n",
        "                # ë°ì´í„° ì¤€ë¹„\n",
        "                documents = [chunk[\"content\"] for chunk in batch]\n",
        "                metadatas = [chunk[\"metadata\"] for chunk in batch]\n",
        "                ids = [f\"chunk_{i}_{j}\" for j in range(len(batch))]\n",
        "                \n",
        "                # ë²¡í„° DBì— ì¶”ê°€\n",
        "                self.collection.add(\n",
        "                    documents=documents,\n",
        "                    metadatas=metadatas,\n",
        "                    ids=ids\n",
        "                )\n",
        "                \n",
        "                print(f\"   âœ… ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì™„ë£Œ ({len(batch)}ê°œ ì²­í¬)\")\n",
        "            \n",
        "            print(f\"ğŸ’¾ ì´ {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ë²¡í„° DB ì €ì¥ ì˜¤ë¥˜: {str(e)}\")\n",
        "    \n",
        "    def search_similar(self, query: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
        "        try:\n",
        "            results = self.collection.query(\n",
        "                query_texts=[query],\n",
        "                n_results=top_k\n",
        "            )\n",
        "            \n",
        "            # ê²°ê³¼ ì •ë¦¬\n",
        "            search_results = []\n",
        "            for i in range(len(results['documents'][0])):\n",
        "                search_results.append({\n",
        "                    \"content\": results['documents'][0][i],\n",
        "                    \"metadata\": results['metadatas'][0][i],\n",
        "                    \"distance\": results['distances'][0][i]\n",
        "                })\n",
        "            \n",
        "            return search_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    def get_collection_info(self):\n",
        "        \"\"\"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ\"\"\"\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            print(f\"ğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´:\")\n",
        "            print(f\"   - ì´ë¦„: {self.collection_name}\")\n",
        "            print(f\"   - ì´ ë¬¸ì„œ ìˆ˜: {count}\")\n",
        "            \n",
        "            return count\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}\")\n",
        "            return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì‹¤í–‰\n",
        "\n",
        "ì•„ë˜ ì…€ì—ì„œ Google Driveì˜ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\n",
        "\n",
        "**ê²½ë¡œ ì„¤ì • ë°©ë²•:**\n",
        "- Google Driveì˜ JSON íŒŒì¼ ê²½ë¡œ: `/content/drive/MyDrive/í´ë”ëª…/íŒŒì¼ëª….json`\n",
        "- ì˜ˆì‹œ: `/content/drive/MyDrive/processed_data/ì•½ê´€_pages.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“˜ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹œìŠ¤í…œ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Google Driveì—ì„œ JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "# ì•„ë˜ ê²½ë¡œë¥¼ ë³¸ì¸ì˜ Google Drive íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”\n",
        "json_file = \"/content/drive/MyDrive/processed_data/ì•½ê´€_pages.json\"  # ì˜ˆì‹œ ê²½ë¡œ\n",
        "\n",
        "# ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ì„¤ì • (Google Driveì— ì €ì¥í• ì§€ ì„ íƒ)\n",
        "# True: Google Driveì— ì €ì¥, False: Colab ì‘ì—… ë””ë ‰í† ë¦¬ì— ì €ì¥\n",
        "save_to_drive = True\n",
        "\n",
        "if save_to_drive:\n",
        "    # Google Driveì— ì €ì¥\n",
        "    chroma_db_path = \"/content/drive/MyDrive/chroma_db\"\n",
        "else:\n",
        "    # Colab ì‘ì—… ë””ë ‰í† ë¦¬ì— ì €ì¥\n",
        "    chroma_db_path = \"./chroma_db\"\n",
        "\n",
        "if not os.path.exists(json_file):\n",
        "    print(f\"âŒ ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_file}\")\n",
        "    print(\"ğŸ’¡ ë¨¼ì € pdf_preprocessor.ipynbë¥¼ ì‹¤í–‰í•˜ì—¬ JSON íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.\")\n",
        "    print(f\"   ë˜ëŠ” Google Driveì˜ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
        "else:\n",
        "    # 2. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (Google Drive ê²½ë¡œ ì§€ì •)\n",
        "    vector_manager = VectorStore(\n",
        "        collection_name=\"insurance_terms\", \n",
        "        api_key=api_key,\n",
        "        chroma_db_path=chroma_db_path\n",
        "    )\n",
        "    \n",
        "    # 3. ë°ì´í„° ë¡œë“œ\n",
        "    pages_data = vector_manager.load_processed_data(json_file)\n",
        "    if not pages_data:\n",
        "        print(\"âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        # 4. í…ìŠ¤íŠ¸ ì²­í‚¹\n",
        "        chunks = vector_manager.process_all_pages(pages_data)\n",
        "        if not chunks:\n",
        "            print(\"âŒ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        else:\n",
        "            # 5. ë²¡í„° DBì— ì €ì¥\n",
        "            vector_manager.store_in_vector_db(chunks)\n",
        "            \n",
        "            # 6. ì €ì¥ì†Œ ì •ë³´ í™•ì¸\n",
        "            vector_manager.get_collection_info()\n",
        "            \n",
        "            print(f\"\\nğŸ‰ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ!\")\n",
        "            print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {chroma_db_path}\")\n",
        "            print(f\"ğŸ“š ì»¬ë ‰ì…˜: {vector_manager.collection_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)\n",
        "\n",
        "ë²¡í„° DBì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ìœ„ì—ì„œ vector_managerê°€ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰)\n",
        "# ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”\n",
        "query = \"ë³´í—˜ê¸ˆ ì§€ê¸‰\"\n",
        "\n",
        "try:\n",
        "    results = vector_manager.search_similar(query, top_k=3)\n",
        "    \n",
        "    print(f\"ğŸ” ê²€ìƒ‰ì–´: '{query}'\")\n",
        "    print(f\"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ\\n\")\n",
        "    \n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"--- ê²°ê³¼ {i} ---\")\n",
        "        print(f\"í˜ì´ì§€: {result['metadata']['page']}\")\n",
        "        print(f\"ê±°ë¦¬: {result['distance']:.4f}\")\n",
        "        print(f\"ë‚´ìš© (ì²« 200ì): {result['content'][:200]}...\")\n",
        "        print()\n",
        "except NameError:\n",
        "    print(\"âš ï¸ vector_managerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
